{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.3318886458873749</th>\n",
       "      <th>0.9484314918518066</th>\n",
       "      <th>7.005006796845237e-09</th>\n",
       "      <th>0.37616536021232605</th>\n",
       "      <th>0.9027106761932373</th>\n",
       "      <th>0.010825136676430702</th>\n",
       "      <th>0.391421377658844</th>\n",
       "      <th>0.8205235600471497</th>\n",
       "      <th>0.009029608219861984</th>\n",
       "      <th>0.3718055486679077</th>\n",
       "      <th>...</th>\n",
       "      <th>0.29954081773757935</th>\n",
       "      <th>0.7083498239517212</th>\n",
       "      <th>-0.01579945534467697</th>\n",
       "      <th>0.3166516125202179</th>\n",
       "      <th>0.7142907977104187</th>\n",
       "      <th>0.003882608376443386</th>\n",
       "      <th>0.32891201972961426</th>\n",
       "      <th>0.7320793867111206</th>\n",
       "      <th>0.016028903424739838</th>\n",
       "      <th>Thumbs up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.331719</td>\n",
       "      <td>0.836966</td>\n",
       "      <td>1.425655e-08</td>\n",
       "      <td>0.290143</td>\n",
       "      <td>0.737632</td>\n",
       "      <td>0.025220</td>\n",
       "      <td>0.293825</td>\n",
       "      <td>0.642307</td>\n",
       "      <td>0.026389</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.698367</td>\n",
       "      <td>-0.020801</td>\n",
       "      <td>0.436264</td>\n",
       "      <td>0.720417</td>\n",
       "      <td>-0.002251</td>\n",
       "      <td>0.420039</td>\n",
       "      <td>0.736897</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>Thumbs up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.317056</td>\n",
       "      <td>0.887811</td>\n",
       "      <td>-5.421682e-09</td>\n",
       "      <td>0.374159</td>\n",
       "      <td>0.864620</td>\n",
       "      <td>-0.018009</td>\n",
       "      <td>0.414538</td>\n",
       "      <td>0.809589</td>\n",
       "      <td>-0.035228</td>\n",
       "      <td>0.432841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307370</td>\n",
       "      <td>0.615218</td>\n",
       "      <td>-0.072040</td>\n",
       "      <td>0.320318</td>\n",
       "      <td>0.581767</td>\n",
       "      <td>-0.075076</td>\n",
       "      <td>0.331136</td>\n",
       "      <td>0.560686</td>\n",
       "      <td>-0.077568</td>\n",
       "      <td>Thumbs up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.316793</td>\n",
       "      <td>0.822169</td>\n",
       "      <td>-2.082952e-09</td>\n",
       "      <td>0.333808</td>\n",
       "      <td>0.720072</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.358782</td>\n",
       "      <td>0.625987</td>\n",
       "      <td>-0.006047</td>\n",
       "      <td>0.385733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441671</td>\n",
       "      <td>0.724965</td>\n",
       "      <td>-0.151641</td>\n",
       "      <td>0.438761</td>\n",
       "      <td>0.734050</td>\n",
       "      <td>-0.127273</td>\n",
       "      <td>0.417806</td>\n",
       "      <td>0.737627</td>\n",
       "      <td>-0.111396</td>\n",
       "      <td>Thumbs up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.335074</td>\n",
       "      <td>0.760724</td>\n",
       "      <td>-6.393500e-09</td>\n",
       "      <td>0.371020</td>\n",
       "      <td>0.634123</td>\n",
       "      <td>-0.003408</td>\n",
       "      <td>0.404557</td>\n",
       "      <td>0.524033</td>\n",
       "      <td>-0.048192</td>\n",
       "      <td>0.438101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454464</td>\n",
       "      <td>0.740203</td>\n",
       "      <td>-0.228856</td>\n",
       "      <td>0.444625</td>\n",
       "      <td>0.744688</td>\n",
       "      <td>-0.199359</td>\n",
       "      <td>0.416158</td>\n",
       "      <td>0.739041</td>\n",
       "      <td>-0.180072</td>\n",
       "      <td>Thumbs up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.335320</td>\n",
       "      <td>0.738061</td>\n",
       "      <td>-3.996900e-09</td>\n",
       "      <td>0.383962</td>\n",
       "      <td>0.625122</td>\n",
       "      <td>-0.007977</td>\n",
       "      <td>0.411422</td>\n",
       "      <td>0.493337</td>\n",
       "      <td>-0.052172</td>\n",
       "      <td>0.420203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455185</td>\n",
       "      <td>0.735981</td>\n",
       "      <td>-0.236878</td>\n",
       "      <td>0.448058</td>\n",
       "      <td>0.742631</td>\n",
       "      <td>-0.206561</td>\n",
       "      <td>0.417384</td>\n",
       "      <td>0.734552</td>\n",
       "      <td>-0.184984</td>\n",
       "      <td>Thumbs up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>0.661404</td>\n",
       "      <td>0.977293</td>\n",
       "      <td>-1.744254e-08</td>\n",
       "      <td>0.601590</td>\n",
       "      <td>0.961294</td>\n",
       "      <td>-0.048898</td>\n",
       "      <td>0.542673</td>\n",
       "      <td>0.928016</td>\n",
       "      <td>-0.087590</td>\n",
       "      <td>0.504118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715800</td>\n",
       "      <td>0.678713</td>\n",
       "      <td>-0.105230</td>\n",
       "      <td>0.724769</td>\n",
       "      <td>0.625894</td>\n",
       "      <td>-0.121379</td>\n",
       "      <td>0.729550</td>\n",
       "      <td>0.574106</td>\n",
       "      <td>-0.134405</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>0.645571</td>\n",
       "      <td>0.964795</td>\n",
       "      <td>-2.200187e-08</td>\n",
       "      <td>0.585681</td>\n",
       "      <td>0.947191</td>\n",
       "      <td>-0.053130</td>\n",
       "      <td>0.522687</td>\n",
       "      <td>0.913205</td>\n",
       "      <td>-0.093566</td>\n",
       "      <td>0.481162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704365</td>\n",
       "      <td>0.649797</td>\n",
       "      <td>-0.093382</td>\n",
       "      <td>0.719536</td>\n",
       "      <td>0.599991</td>\n",
       "      <td>-0.105929</td>\n",
       "      <td>0.730448</td>\n",
       "      <td>0.550683</td>\n",
       "      <td>-0.116846</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>0.635185</td>\n",
       "      <td>0.956771</td>\n",
       "      <td>-2.206488e-08</td>\n",
       "      <td>0.570620</td>\n",
       "      <td>0.939585</td>\n",
       "      <td>-0.056860</td>\n",
       "      <td>0.507052</td>\n",
       "      <td>0.909483</td>\n",
       "      <td>-0.101592</td>\n",
       "      <td>0.465908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702134</td>\n",
       "      <td>0.640448</td>\n",
       "      <td>-0.110649</td>\n",
       "      <td>0.720716</td>\n",
       "      <td>0.591574</td>\n",
       "      <td>-0.125855</td>\n",
       "      <td>0.733749</td>\n",
       "      <td>0.542532</td>\n",
       "      <td>-0.138943</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>0.632230</td>\n",
       "      <td>0.948478</td>\n",
       "      <td>-2.140310e-08</td>\n",
       "      <td>0.564375</td>\n",
       "      <td>0.928532</td>\n",
       "      <td>-0.053547</td>\n",
       "      <td>0.500441</td>\n",
       "      <td>0.894974</td>\n",
       "      <td>-0.096905</td>\n",
       "      <td>0.457151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701745</td>\n",
       "      <td>0.630448</td>\n",
       "      <td>-0.113921</td>\n",
       "      <td>0.721380</td>\n",
       "      <td>0.581436</td>\n",
       "      <td>-0.127506</td>\n",
       "      <td>0.734889</td>\n",
       "      <td>0.532162</td>\n",
       "      <td>-0.138742</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>0.625451</td>\n",
       "      <td>0.939667</td>\n",
       "      <td>-2.083128e-08</td>\n",
       "      <td>0.557644</td>\n",
       "      <td>0.918518</td>\n",
       "      <td>-0.055520</td>\n",
       "      <td>0.495509</td>\n",
       "      <td>0.883251</td>\n",
       "      <td>-0.099351</td>\n",
       "      <td>0.454860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698487</td>\n",
       "      <td>0.619800</td>\n",
       "      <td>-0.111575</td>\n",
       "      <td>0.719564</td>\n",
       "      <td>0.572947</td>\n",
       "      <td>-0.125228</td>\n",
       "      <td>0.733782</td>\n",
       "      <td>0.524869</td>\n",
       "      <td>-0.136915</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1587 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.3318886458873749   0.9484314918518066   7.005006796845237e-09  \\\n",
       "0               0.331719             0.836966            1.425655e-08   \n",
       "1               0.317056             0.887811           -5.421682e-09   \n",
       "2               0.316793             0.822169           -2.082952e-09   \n",
       "3               0.335074             0.760724           -6.393500e-09   \n",
       "4               0.335320             0.738061           -3.996900e-09   \n",
       "...                  ...                  ...                     ...   \n",
       "1582            0.661404             0.977293           -1.744254e-08   \n",
       "1583            0.645571             0.964795           -2.200187e-08   \n",
       "1584            0.635185             0.956771           -2.206488e-08   \n",
       "1585            0.632230             0.948478           -2.140310e-08   \n",
       "1586            0.625451             0.939667           -2.083128e-08   \n",
       "\n",
       "       0.37616536021232605   0.9027106761932373   0.010825136676430702  \\\n",
       "0                 0.290143             0.737632               0.025220   \n",
       "1                 0.374159             0.864620              -0.018009   \n",
       "2                 0.333808             0.720072               0.019272   \n",
       "3                 0.371020             0.634123              -0.003408   \n",
       "4                 0.383962             0.625122              -0.007977   \n",
       "...                    ...                  ...                    ...   \n",
       "1582              0.601590             0.961294              -0.048898   \n",
       "1583              0.585681             0.947191              -0.053130   \n",
       "1584              0.570620             0.939585              -0.056860   \n",
       "1585              0.564375             0.928532              -0.053547   \n",
       "1586              0.557644             0.918518              -0.055520   \n",
       "\n",
       "       0.391421377658844   0.8205235600471497   0.009029608219861984  \\\n",
       "0               0.293825             0.642307               0.026389   \n",
       "1               0.414538             0.809589              -0.035228   \n",
       "2               0.358782             0.625987              -0.006047   \n",
       "3               0.404557             0.524033              -0.048192   \n",
       "4               0.411422             0.493337              -0.052172   \n",
       "...                  ...                  ...                    ...   \n",
       "1582            0.542673             0.928016              -0.087590   \n",
       "1583            0.522687             0.913205              -0.093566   \n",
       "1584            0.507052             0.909483              -0.101592   \n",
       "1585            0.500441             0.894974              -0.096905   \n",
       "1586            0.495509             0.883251              -0.099351   \n",
       "\n",
       "       0.3718055486679077  ...   0.29954081773757935   0.7083498239517212  \\\n",
       "0                0.327682  ...              0.457627             0.698367   \n",
       "1                0.432841  ...              0.307370             0.615218   \n",
       "2                0.385733  ...              0.441671             0.724965   \n",
       "3                0.438101  ...              0.454464             0.740203   \n",
       "4                0.420203  ...              0.455185             0.735981   \n",
       "...                   ...  ...                   ...                  ...   \n",
       "1582             0.504118  ...              0.715800             0.678713   \n",
       "1583             0.481162  ...              0.704365             0.649797   \n",
       "1584             0.465908  ...              0.702134             0.640448   \n",
       "1585             0.457151  ...              0.701745             0.630448   \n",
       "1586             0.454860  ...              0.698487             0.619800   \n",
       "\n",
       "       -0.01579945534467697   0.3166516125202179   0.7142907977104187  \\\n",
       "0                 -0.020801             0.436264             0.720417   \n",
       "1                 -0.072040             0.320318             0.581767   \n",
       "2                 -0.151641             0.438761             0.734050   \n",
       "3                 -0.228856             0.444625             0.744688   \n",
       "4                 -0.236878             0.448058             0.742631   \n",
       "...                     ...                  ...                  ...   \n",
       "1582              -0.105230             0.724769             0.625894   \n",
       "1583              -0.093382             0.719536             0.599991   \n",
       "1584              -0.110649             0.720716             0.591574   \n",
       "1585              -0.113921             0.721380             0.581436   \n",
       "1586              -0.111575             0.719564             0.572947   \n",
       "\n",
       "       0.003882608376443386   0.32891201972961426   0.7320793867111206  \\\n",
       "0                 -0.002251              0.420039             0.736897   \n",
       "1                 -0.075076              0.331136             0.560686   \n",
       "2                 -0.127273              0.417806             0.737627   \n",
       "3                 -0.199359              0.416158             0.739041   \n",
       "4                 -0.206561              0.417384             0.734552   \n",
       "...                     ...                   ...                  ...   \n",
       "1582              -0.121379              0.729550             0.574106   \n",
       "1583              -0.105929              0.730448             0.550683   \n",
       "1584              -0.125855              0.733749             0.542532   \n",
       "1585              -0.127506              0.734889             0.532162   \n",
       "1586              -0.125228              0.733782             0.524869   \n",
       "\n",
       "       0.016028903424739838  Thumbs up  \n",
       "0                  0.005397  Thumbs up  \n",
       "1                 -0.077568  Thumbs up  \n",
       "2                 -0.111396  Thumbs up  \n",
       "3                 -0.180072  Thumbs up  \n",
       "4                 -0.184984  Thumbs up  \n",
       "...                     ...        ...  \n",
       "1582              -0.134405    awesome  \n",
       "1583              -0.116846    awesome  \n",
       "1584              -0.138943    awesome  \n",
       "1585              -0.138742    awesome  \n",
       "1586              -0.136915    awesome  \n",
       "\n",
       "[1587 rows x 64 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.31719249e-01,  8.36965561e-01,  1.42565524e-08, ...,\n",
       "         4.20039147e-01,  7.36897111e-01,  5.39740035e-03],\n",
       "       [ 3.17056447e-01,  8.87811244e-01, -5.42168221e-09, ...,\n",
       "         3.31135690e-01,  5.60686350e-01, -7.75684491e-02],\n",
       "       [ 3.16793442e-01,  8.22169423e-01, -2.08295159e-09, ...,\n",
       "         4.17806178e-01,  7.37627268e-01, -1.11395769e-01],\n",
       "       ...,\n",
       "       [ 6.35184884e-01,  9.56771016e-01, -2.20648797e-08, ...,\n",
       "         7.33749032e-01,  5.42531550e-01, -1.38943240e-01],\n",
       "       [ 6.32230461e-01,  9.48478460e-01, -2.14031033e-08, ...,\n",
       "         7.34889030e-01,  5.32161891e-01, -1.38742402e-01],\n",
       "       [ 6.25450730e-01,  9.39666510e-01, -2.08312798e-08, ...,\n",
       "         7.33781874e-01,  5.24868727e-01, -1.36915445e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thumbs up', 'Thumbs up', 'Thumbs up', ..., 'awesome', 'awesome',\n",
       "       'awesome'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1269, 63)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 63)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1269,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier1 = LogisticRegression()\n",
    "classifier1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier2 = DecisionTreeClassifier()\n",
    "classifier2.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier3 = RandomForestClassifier()\n",
    "classifier3.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9968553459119497\n",
      "0.9842767295597484\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Y_pred1=classifier1.predict(X_test)\n",
    "Y_pred2=classifier2.predict(X_test)\n",
    "Y_pred3=classifier3.predict(X_test)\n",
    "\n",
    "print(accuracy_score(Y_pred1,Y_test))\n",
    "print(accuracy_score(Y_pred2,Y_test))\n",
    "print(accuracy_score(Y_pred3,Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = open('model.pkl','wb')\n",
    "pickle.dump(classifier1,model)\n",
    "model.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
